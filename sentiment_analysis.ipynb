{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6c2470-05ff-4899-bc1e-0978b6786133",
   "metadata": {},
   "source": [
    "# Look at Big Picture\n",
    "\n",
    "IMDB dataset for sentiment analysis -> 0 for negative, 1 for positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbae1b56-7cb4-4d2a-87a2-19d3bbbafa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd65913-2a5b-411b-9bcf-aceea6710653",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# to use DL\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      5\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# to use DL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f9f4c-397e-4def-8ca2-644427a002e8",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc70da-ac41-435e-86f6-31cc4c6ee258",
   "metadata": {},
   "source": [
    "## Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3b06d-e801-4ee7-adfd-67e6dd3ed23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloaded from kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7a4ab-684e-44eb-b15e-52e817c67afa",
   "metadata": {},
   "source": [
    "## Take a Quick Look at Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50717c38-3a8e-4b91-84f8-426573c7bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('movie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480fbd24-8169-49e8-8691-33b2125a392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb02de1-940f-4698-9b7c-83d5df21e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42f485-44a9-43a8-8bba-d1a15cf64d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe42fda3-1cdb-4541-a694-0ed2bb0613c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "dataset.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8adfa-a917-45ba-840d-ae3f6d00c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "dataset = dataset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318fe14-e7d9-4b40-b530-4b71ecbe5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct values in label and its count\n",
    "dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ccb6ea-7996-45a7-aba5-6d3e3511525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# both are nearly equal so accuracy is a good metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1377b49c-1847-494b-ba9b-8ddd13957ef4",
   "metadata": {},
   "source": [
    "## Create Train, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b968597-b74d-4b31-b8bc-fa8033800a43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[0;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = dataset['text'] \n",
    "y = dataset['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3e747-a20c-4f2d-8b64-a7d26e0dd11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c988ab1-b79c-4528-8de6-1f5b9c14e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044ac88-90e2-4788-a12c-bf2439457f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281b689-70a7-4fee-b337-d58422c22c6d",
   "metadata": {},
   "source": [
    "# Discovering & Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc4493-98c3-4d41-88c8-8b7db48e9002",
   "metadata": {},
   "source": [
    "## Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15672be-3078-4ed5-b641-62ac8a358ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of label counts in training set\n",
    "sns.countplot(x=y_train)\n",
    "plt.title(\"Label Distribution in Training Set\")\n",
    "plt.xlabel(\"Sentiment (0 = Negative, 1 = Positive)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ae547-f3e1-4128-83c3-ad7664887a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Combine all text in X_train\n",
    "all_text = ' '.join(X_train)\n",
    "\n",
    "# Tokenize: remove non-alphabetic characters and lowercase everything\n",
    "words = re.findall(r'\\b[a-z]+\\b', all_text.lower())\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e79a1d-de64-4702-b9eb-b67a319d6398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 20 most common words\n",
    "words_rep = 20\n",
    "most_common_words = word_freq.most_common(words_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de942bc-a172-4ed7-b79a-cfd2691e6bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart \n",
    "words_bar, freqs_bar = zip(*most_common_words)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(words_bar, freqs_bar, color='skyblue')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 20 Most Frequent Words in X_train')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c760768-5377-4a16-8e13-cfd4c749afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of X_train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf827c-83bd-4184-8baf-320a3adb5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see non stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "word_freq = Counter(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcf175a-37c3-474b-88c8-c6f1ada7077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words = word_freq.most_common(words_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47dc24-f912-493e-a90d-b9cb6f9caf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart\n",
    "words_bar, freqs_bar = zip(*most_common_words)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(words_bar, freqs_bar, color='salmon')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 20 Most Frequent Words in X_train (No Stopwords)')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6556bc2b-34cb-4262-8502-1b43bd74a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of X_train (No Stopwords)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a319603-202b-46af-8287-64e9ae42b556",
   "metadata": {},
   "source": [
    "# Prepare Data for ML Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ad88a-d9ba-43de-8fe0-b1d3c534559c",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c98aa-69e3-4511-99cb-4a21e1149821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not req."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cbe113-4aa9-45d3-a11f-9ebf0eb42391",
   "metadata": {},
   "source": [
    "## Handling Text Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a624a9-bb3c-4792-813c-b9b559312757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X.apply(self._clean_text)\n",
    "    \n",
    "    def _clean_text(self, text):\n",
    "        # Remove HTML\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b282419-c1ec-4273-9358-71a6229de0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = TextCleaner()\n",
    "\n",
    "X_train_clean = cleaner.transform(X_train)\n",
    "X_valid_clean = cleaner.transform(X_valid)\n",
    "X_test_clean  = cleaner.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183c4d2-75c7-4540-a98c-ee10892fef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use embedding layer after applying transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e868f0-271c-441b-b509-f370e7e6284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_clean)\n",
    "X_valid_seq = tokenizer.texts_to_sequences(X_valid_clean)\n",
    "X_test_seq  = tokenizer.texts_to_sequences(X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778b5b8-0d7b-4615-b38d-c2e9d4ea555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 200\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_valid_pad = pad_sequences(X_valid_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_pad  = pad_sequences(X_test_seq,  maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03e46d-7f31-4f08-ac74-5a60ee3a208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'i like the movie so much <br> it is excallent'\n",
    "\n",
    "# Step 1: Clean the text\n",
    "sample_cleaned = cleaner.transform(pd.Series([sample]))[0]\n",
    "\n",
    "# Step 2: Tokenize\n",
    "sample_seq = tokenizer.texts_to_sequences([sample_cleaned])[0]\n",
    "\n",
    "# Step 3: Pad\n",
    "sample_pad = pad_sequences([sample_seq], maxlen=200, padding='post', truncating='post')[0]\n",
    "\n",
    "# Output\n",
    "print(\"🔹 Original:\", sample)\n",
    "print(\"🔹 Cleaned:\", sample_cleaned)\n",
    "print(\"🔹 Tokenized:\", sample_seq)\n",
    "print(\"🔹 Padded:\", sample_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddaab7d-967e-4a1d-b873-ebce416de67b",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689fe21-4af4-4414-949e-6c13691efc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no feature scaling required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b959b50c-0c73-487a-a7be-4c81fbdada13",
   "metadata": {},
   "source": [
    "## Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a3f68-7863-4f10-84ec-2d68b9b70b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final data for ml model is in X_train_pad, X_valid_pad, X_test_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6501a-b097-42a4-b449-5b532cb2b0f5",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a1f924-3c66-4495-9547-4e7001221c32",
   "metadata": {},
   "source": [
    "## Create Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec54482-dc3f-4ec7-ab1a-15124e7e9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use glove.6B.50d.txt (50-dimensional vectors)\n",
    "\n",
    "embedding_index = {}\n",
    "with open('glove.6B.50d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad55e1-52ae-4764-9879-a7d825fab577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding matrix\n",
    "embedding_dim = 50\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(10000, len(word_index) + 1)\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < num_words:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018757cb-39ff-4f15-859f-8d8494e66340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embeddings = Embedding(input_dim=num_words, output_dim=embedding_dim,\n",
    "                        weights=[embedding_matrix], input_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf9619d-73da-4d54-9f84-e663a4ee960b",
   "metadata": {},
   "source": [
    "## Create & Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdfa7e8-faa9-436f-970f-c7d620935915",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bccdd7b-940f-461e-81c3-f13f75d77dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    embeddings,\n",
    "    LSTM(100, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "# Freeze the embedding layer\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a095a-a309-46e3-bb84-4a486ea32f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=1e-4), \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af78310-094f-4360-b05d-7060154f8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train_pad, y_train,\n",
    "                    validation_data=(X_valid_pad, y_valid),\n",
    "                    epochs=10, batch_size=32,\n",
    "                    callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f87e0-4a46-47bb-b98e-21f7f145410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_pad, y_train,\n",
    "                    validation_data=(X_valid_pad, y_valid),\n",
    "                    epochs=20, batch_size=32,\n",
    "                    callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b4bf0-b3bb-4eb6-889f-61b79db90a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('best_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd52958-5151-42ce-8c68-7856fd4edc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets unfreeze embegging layer\n",
    "model.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ce2499-5fe7-41a3-bbaf-ffdd80c9ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8164dd7-dedc-403d-9dc1-5324454c70d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(X_train_pad, y_train,\n",
    "                      validation_data=(X_valid_pad, y_valid),\n",
    "                      epochs=10, batch_size=32,\n",
    "                      callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610196cc-3050-4d61-ad6d-880dbf00ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets train our final model for other 10 epochs\n",
    "history = model.fit(X_train_pad, y_train,\n",
    "                      validation_data=(X_valid_pad, y_valid),\n",
    "                      epochs=10, batch_size=32,\n",
    "                      callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff597fb7-4277-425c-bc7b-cd6aff711eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model = load_model('best_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cfee5b-1056-4a03-af7d-5b98b2effdba",
   "metadata": {},
   "source": [
    "# Evaluate Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b19d3e-38dd-44d3-9c7f-0979e39f97f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_pad, y_test, verbose=1)\n",
    "\n",
    "print(f\"\\n Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\" Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
